{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load news data from database into dataframe anc cluster with https://towardsdatascience.com/all-the-news-17fa34b52b9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/data-app/app/database\n",
      "database\n",
      ".\n",
      "/workspaces/data-app/app/news_clustering\n",
      "/usr/local/lib/python311.zip\n",
      "/usr/local/lib/python3.11\n",
      "/usr/local/lib/python3.11/lib-dynload\n",
      "\n",
      "/home/vscode/.local/lib/python3.11/site-packages\n",
      "/usr/local/lib/python3.11/site-packages\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/workspaces/data-app/app/database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "from database import Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Database()\n",
    "database.open_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1052/2993504995.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  articles = sqlio.read_sql_query(sql, database.connection)\n"
     ]
    }
   ],
   "source": [
    "## get articles table into datframe\n",
    "sql = \"SELECT * FROM articles;\"\n",
    "articles = sqlio.read_sql_query(sql, database.connection)\n",
    "database.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   urlid              49 non-null     object        \n",
      " 1   headline           45 non-null     object        \n",
      " 2   content            49 non-null     object        \n",
      " 3   authors            49 non-null     object        \n",
      " 4   uploadtimestamp    38 non-null     datetime64[ns]\n",
      " 5   imageurl           49 non-null     object        \n",
      " 6   imagedescription   49 non-null     object        \n",
      " 7   scrapingtimestamp  49 non-null     datetime64[ns]\n",
      " 8   source             49 non-null     object        \n",
      " 9   topic              49 non-null     object        \n",
      "dtypes: datetime64[ns](2), object(8)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "articles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "### 1) stemming\n",
    "### 2) create word matrix\n",
    "### 3) TFIDF\n",
    "### 4) Clustering\n",
    "### 5) majority vote of category within clusters to be used as cluster name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analytics preprocessing\n",
    "\n",
    "# Option A\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]  # Stem words\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_lemmatizer(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Stem words\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post hostag famili urg return negoti tabl rel hostag taken israel hama call isra govern return negoti tabl price promis us price would return hostag daniel lifshitz whose grandpar yochev ode lifshitz taken hostag among famili member speak news confer morn call cabinet meet order forget feel forgot time everyth except famili ignor indiffer lack attent us disgrac said reuter handout pictur copyright reuter handout pictur yochev lifshitz husband ode yochev freed ode still captiv imag caption yochev lifshitz husband ode yochev freed ode still captiv haim yitzhak brother avinatan kidnap supernova festiv told report famili dont think interven tactic manag war want answer promis us would meet us didnt happen dont know isra prime minist offic respond grow frustrat famili meet famili hostag war cabinet alreadi schedul yesterday wednesday famili request possibl reschedul earlier examin articl share tool share view share option share post copi link read link\n"
     ]
    }
   ],
   "source": [
    "preprocessed_articles = articles.content.apply(lambda x: preprocess(x))\n",
    "print(preprocessed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posted hostage family urge return negotiating table relative hostage taken israel hamas calling israeli government return negotiating table price promised u price would return hostage daniel lifshitz whose grandparent yocheved oded lifshitz taken hostage among family member speaking news conference morning call cabinet meet order forget feel forgot time everything except family ignoring indifference lack attention u disgrace said reuters handout picture copyright reuters handout picture yocheved lifshitz husband oded yocheved freed oded still captivity image caption yocheved lifshitz husband oded yocheved freed oded still captivity haim yitzhak brother avinatan kidnapped supernova festival told reporter family dont think intervening tactical management war want answer promised u would meet u didnt happen dont know israeli prime minister office responded growing frustration family meeting family hostage war cabinet already scheduled yesterday wednesday family request possibility rescheduling earlier examined article share tool share view share option share post copy link read link\n"
     ]
    }
   ],
   "source": [
    "preprocessed_lemmatized_articles = articles.content.apply(lambda x: preprocess_lemmatizer(x))\n",
    "print(preprocessed_lemmatized_articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo: first add cnn articles to DB and rerun code\n",
    "### second choose one way of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.01918833 0.04558346 ... 0.         0.01838554 0.01574456]\n",
      " [0.01918833 1.         0.05203623 ... 0.00818932 0.02765311 0.02653302]\n",
      " [0.04558346 0.05203623 1.         ... 0.         0.04630605 0.05853878]\n",
      " ...\n",
      " [0.         0.00818932 0.         ... 1.         0.02684336 0.        ]\n",
      " [0.01838554 0.02765311 0.04630605 ... 0.02684336 1.         0.11560519]\n",
      " [0.01574456 0.02653302 0.05853878 ... 0.         0.11560519 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Transform into numerical data and apply cosine similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_articles)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from here down: to be deleted!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) stemming\n",
    "stemmer = PorterStemmer()\n",
    "##tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "progress = 0 #for keeping track of where the function is\n",
    "\n",
    "def stemm(x):\n",
    "   end = time.time()\n",
    "   dirty = word_tokenize(x)\n",
    "   tokens = []\n",
    "   for word in dirty:\n",
    "      if word.strip('.') == '': #this deals with the bug\n",
    "         pass\n",
    "      elif re.search(r'\\d{1,}', word): #getting rid of digits\n",
    "         pass\n",
    "      else:\n",
    "         tokens.append(word.strip('.'))\n",
    "   global start\n",
    "   global progress\n",
    "   tokens = pos_tag(tokens) #\n",
    "   progress += 1\n",
    "   stems = ' '.join(stemmer.stem(key.lower()) for key, value in  tokens if value != 'NNP') #getting rid of proper nouns\n",
    "   end = time.time()\n",
    "   sys.stdout.write('\\r {} percent, {} position, {} per second '.format(str(float(progress / len(articles))), str(progress), (1 / (end - start)))) #lets us see how much time is left \n",
    "   start = time.time()\n",
    "   return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# articles['stems'] = articles.content.apply(lambda x: stemm(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
